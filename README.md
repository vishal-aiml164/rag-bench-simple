# ğŸ“š RAG-Bench

**RAG-Bench** is a modular benchmarking framework for **Retrieval-Augmented Generation (RAG)** experiments.  
It provides flexible pipelines for running, evaluating, and comparing different retrievers, RAG variants, and evaluation strategies â€” all configurable via YAML.

---

## âœ¨ Features

- ğŸ” **Retrievers**
  - FAISS
  - BM25
  - Hybrid (dense + sparse)
  - Qdrant

- ğŸ§© **RAG Variants**
  - Naive RAG
  - Corrective RAG
  - Speculative RAG
  - Graph RAG
  - LightRAG

- ğŸ“ **Evaluators**
  - **IR Evaluator**: classic information retrieval metrics (recall, precision, MRR).
  - **RAGAS Embedding Evaluator**: evaluates context & answer relevancy using embeddings.
  - **RAGAS LLM Evaluator**: uses an LLM-as-a-judge (faithfulness, factuality, coherence, fluency).

- âš¡ **Config-driven experiments**  
  All runs are defined in `config/experiments.yaml` and reproducible with a single command.

- ğŸ“Š **Visualization utilities**  
  Automatically generate plots of evaluation metrics for comparison.

---

## ğŸ“‚ Project Structure

rag-bench/
â”‚
â”œâ”€â”€ README.md # Overview of the project and usage
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt # All dependencies
â”‚
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ experiments.yaml # Config for each experiment run
â”‚ â””â”€â”€ retriever_config.yaml # Vector store / BM25 / hybrid settings
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ input/ # Datasets (CUAD, HotpotQA, SQuAD, etc.)
â”‚ â”œâ”€â”€ output/ # Raw cached indexes & logs
â”‚ â””â”€â”€ results/ # Evaluation results & plots
â”‚
â”œâ”€â”€ rag_variants/ # RAG flavors
â”‚ â”œâ”€â”€ naive_rag.py
â”‚ â”œâ”€â”€ corrective_rag.py
â”‚ â”œâ”€â”€ speculative_rag.py
â”‚ â”œâ”€â”€ graph_rag.py
â”‚ â””â”€â”€ lightrag.py
â”‚
â”œâ”€â”€ retrievers/ # Retriever implementations
â”‚ â”œâ”€â”€ faiss_retriever.py
â”‚ â”œâ”€â”€ bm25_retriever.py
â”‚ â”œâ”€â”€ hybrid_retriever.py
â”‚ â””â”€â”€ qdrant_retriever.py
â”‚
â”œâ”€â”€ evaluators/ # Evaluation modules
â”‚ â”œâ”€â”€ ir_evaluator.py
â”‚ â”œâ”€â”€ ragas_embedding_evaluator.py
â”‚ â”œâ”€â”€ ragas_llm_evaluator.py
â”‚ â””â”€â”€ llm_judge_eval.py
â”‚
â”œâ”€â”€ utils/ # Helpers
â”‚ â”œâ”€â”€ data_loader.py
â”‚ â”œâ”€â”€ chunk_utils.py
â”‚ â”œâ”€â”€ logger.py
â”‚ â”œâ”€â”€ prompt_utils.py
â”‚ â”œâ”€â”€ estimate_chunks_memory.py
â”‚ â”œâ”€â”€ visualizer.py
â”‚
â”œâ”€â”€ notebooks/ # Analysis notebooks
â”‚ â””â”€â”€ analysis.ipynb
â”‚
â”œâ”€â”€ docs/ # Documentation
â”‚ â””â”€â”€ architecture.md
â”‚
â”œâ”€â”€ main.py # Entry point for running single experiments
â””â”€â”€ runners/ # (Optional) automated experiment runners
â”œâ”€â”€ run_experiment.py
â””â”€â”€ search_grid.py


---

## âš™ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/<your-username>/rag-bench.git
cd rag-bench

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)

# Install dependencies
pip install -r requirements.txt


ğŸš€ Usage

Prepare your dataset
Place datasets under data/input/. For example:

data/input/squad/


Configure experiment
Edit config/experiments.yaml:

dataset:
  name: squad
  subset: validation

experiment:
  retriever: faiss
  rag_variant: naive_rag
  embeddings: all-MiniLM-L6-v2
  num_records: 10
  top_k: 5
  chunk_size: 512
  chunk_overlap: 0


Run an experiment

python main.py


View results
Plots and results will be saved under data/results/:

data/results/squad_validation_512_0_results.png

ğŸ“Š Example Output

Average IR metrics (recall, precision, etc.)

RAGAS embedding scores (context relevancy, answer relevancy)

Optional LLM-judge scores (faithfulness, coherence, fluency)

Visual comparison plots

ğŸ› ï¸ Roadmap

 Add HuggingFace vLLM and local GPU LLM inference support.

 Extend to multi-hop QA datasets.

 Add grid-search runner for hyperparameter sweeps.

 Dockerize setup for easy deployment.

 Add Streamlit UI for interactive exploration.

ğŸ“œ License

MIT License. See LICENSE
 for details.

ğŸ¤ Contributing

Contributions are welcome!
Please open issues for bugs/feature requests and submit PRs for enhancements.
